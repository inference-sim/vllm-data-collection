# Benchmark Output Format

This document specifies the output formats and locations for the different scripts used in the vLLM benchmarking process.

## `benchmark_runner_simulator.py`

The `benchmark_runner_simulator.py` script orchestrates the experiments. The benchmark data itself is generated by `benchmark_serving_simulator.py`, and the runner script controls where these outputs are saved.

-   **Output Directory**: The `--output` flag specifies the directory where benchmark output files will be saved.
-   **File Naming**: The output files are named based on the experiment configuration from `config_generator.py`, with a timestamp appended.

The naming convention for an experiment is as follows:
```
exp_{num_prompts}p_{request_rate}r_{temperature}t_{max_num_batched_tokens}mbt_{long_prefill_token_threshold}lpt_{dataset_name}_{model_name}
```
For example: `exp_100p_16r_0.0t_2048mbt_1000000lpt_sharegpt_Qwen_Qwen2.5-0.5B.json`

## vLLM Server Logs

The vLLM server (`vllm.entrypoints.api_server`) logs, which include server startup information and request/response data, are saved to log files.

-   **Log File Location**: These logs are created in the `benchmarks` directory from where the scripts are run.
-   **Naming Format**: The log files are named `vllm_server_{run}.log`, where `{run}` corresponds to the run number of the experiment.

This behavior is defined in `benchmarks/container_entrypoint.py`. You can modify the following lines in the `start_vllm_server` function to change the logging behavior, for example, to change the file path or disable logging to a file:
```python
# in benchmarks/container_entrypoint.py
with open(f'vllm_server_{run}.log', 'w') as log_file:
    # Redirect stdout and stderr to log file
    process = subprocess.Popen(cmd, stdout=log_file, stderr=subprocess.STDOUT)
```

## `benchmark_runner_simulator.py` Logs

The `benchmark_runner_simulator.py` script itself logs the status of each experiment, such as when a server starts, when an experiment begins, and when it completes.

-   **Output Stream**: By default, these logs are printed to the standard output (your terminal).
-   **Redirecting Output**: You can redirect this output to a file using standard shell redirection:
    ```bash
    python benchmarks/benchmark_runner_simulator.py <config_file.yaml> --output <output_directory> > runner_output.log
    ```
-   **Customizing Logs**: To change what is logged, you can edit the `print()` statements within `benchmarks/benchmark_runner_simulator.py`.

## Saving Output Tokens

For instructions on how to configure the benchmarks to save the generated output tokens for each request, please refer to the following documentation:

[How to Save Output Tokens](./saving_output_tokens.md)
