# vLLM Benchmark Configuration

# Model to benchmark


# Single test configuration
test:

  exp1:
    name: handcrafted_exp_1
    description: Handcrafted prompts for model
    model: Qwen/Qwen2-1.5B-Instruct
    result_folder: handcrafted_prompts
    runs: 1
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: false
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: custom
      dataset_path: handcrafted_prompts_exp1.jsonl
      num_prompts: 1
      request_rate: 1
      temperature: 0.0
      output_len: 2
      seed: 42

  exp2:
    name: handcrafted_exp_2
    description: Handcrafted prompts for model
    model: Qwen/Qwen2-1.5B-Instruct
    result_folder: handcrafted_prompts
    runs: 1
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: false
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: custom
      dataset_path: handcrafted_prompts_exp2.jsonl
      num_prompts: 1
      request_rate: 1
      temperature: 0.0
      output_len: 1
      seed: 42

  exp3:
    name: handcrafted_exp_3
    description: Handcrafted prompts for model
    model: Qwen/Qwen2-1.5B-Instruct
    result_folder: handcrafted_prompts
    runs: 1
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: false
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: custom
      dataset_path: handcrafted_prompts_exp3.jsonl
      num_prompts: 2
      temperature: 0.0
      output_len: 2
      seed: 42

  # baseline2:
  #   name: "exp2"
  #   description: "Basic vLLM performance test"
  #   model: "Qwen/Qwen2.5-0.5B"
  #   runs: 1

  #   # vLLM server parameters
  #   vllm:
  #     gpu_memory_utilization: 0.9
  #     enable_prefix_caching: true
  #     disable_log_requests: false
  #     block_size: 16
  #     max_model_len: 2048
  #     max_num_batched_tokens: 2048
  #     max_num_seqs: 256
  #     long_prefill_token_threshold: 1000000
  #     seed: 42

  #   # Benchmark parameters
  #   benchmark:
  #     backend: "vllm"
  #     dataset_name: "sharegpt"
  #     dataset_path: "ShareGPT_V3_unfiltered_cleaned_split.json"
  #     num_prompts: 100
  #     request_rate: 30
  #     sharegpt_output_len: 0 # Set to 0 for no output length limit
  #     temperature: 0.0
  #     seed: 42

  # baseline3:
  #     name: "exp3"
  #     description: "Basic vLLM performance test"
  #     model: "Qwen/Qwen2.5-0.5B"
  #     runs: 1

  #     # vLLM server parameters
  #     vllm:
  #       gpu_memory_utilization: 0.9
  #       enable_prefix_caching: true
  #       disable_log_requests: false
  #       block_size: 16
  #       max_model_len: 2048
  #       max_num_batched_tokens: 2048
  #       max_num_seqs: 256
  #       long_prefill_token_threshold: 1000000
  #       seed: 42

  #     # Benchmark parameters
  #     benchmark:
  #       backend: "vllm"
  #       dataset_name: "sharegpt"
  #       dataset_path: "ShareGPT_V3_unfiltered_cleaned_split.json"
  #       num_prompts: 100
  #       request_rate: 64
  #       sharegpt_output_len: 0 # Set to 0 for no output length limit
  #       temperature: 0.0
  #       seed: 42

  # baseline5:
  #   name: "exp4"
  #   description: "Basic vLLM performance test"
  #   model: "Qwen/Qwen2.5-0.5B"
  #   runs: 1

  #   # vLLM server parameters
  #   vllm:
  #     gpu_memory_utilization: 0.9
  #     enable_prefix_caching: true
  #     disable_log_requests: false
  #     block_size: 16
  #     max_model_len: 2048
  #     max_num_batched_tokens: 2048
  #     max_num_seqs: 256
  #     long_prefill_token_threshold: 1000000
  #     seed: 42

  #   # Benchmark parameters
  #   benchmark:
  #     backend: "vllm"
  #     dataset_name: "sharegpt"
  #     dataset_path: "ShareGPT_V3_unfiltered_cleaned_split.json"
  #     num_prompts: 5000
  #     request_rate: 32
  #     temperature: 0.0
  #     seed: 42
    
  # baseline6:
  #   name: "exp5"
  #   description: "Basic vLLM performance test"
  #   model: "Qwen/Qwen2.5-0.5B"
  #   runs: 1

  #   # vLLM server parameters
  #   vllm:
  #     gpu_memory_utilization: 0.9
  #     enable_prefix_caching: true
  #     disable_log_requests: false
  #     block_size: 16
  #     max_model_len: 2048
  #     max_num_batched_tokens: 2048
  #     max_num_seqs: 256
  #     long_prefill_token_threshold: 1000000
  #     seed: 42

  #   # Benchmark parameters
  #   benchmark:
  #     backend: "vllm"
  #     dataset_name: "sharegpt"
  #     dataset_path: "ShareGPT_V3_unfiltered_cleaned_split.json"
  #     num_prompts: 5000
  #     request_rate: 35
  #     temperature: 0.0
  #     seed: 42

  # baseline7:
  #   name: "exp5"
  #   description: "Basic vLLM performance test"
  #   model: "Qwen/Qwen2.5-0.5B"
  #   runs: 1

  #   # vLLM server parameters
  #   vllm:
  #     gpu_memory_utilization: 0.9
  #     enable_prefix_caching: true
  #     disable_log_requests: false
  #     block_size: 16
  #     max_model_len: 2048
  #     max_num_batched_tokens: 2048
  #     max_num_seqs: 256
  #     long_prefill_token_threshold: 1000000
  #     seed: 42

  #   # Benchmark parameters
  #   benchmark:
  #     backend: "vllm"
  #     dataset_name: "sharegpt"
  #     dataset_path: "ShareGPT_V3_unfiltered_cleaned_split.json"
  #     num_prompts: 5000
  #     request_rate: 40
  #     temperature: 0.0
  #     seed: 42