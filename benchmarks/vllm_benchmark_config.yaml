# vLLM Benchmark Configuration

# Model to benchmark


# Generated test configurations
test:
  baseline:
    name: exp_100p_16r_0.0t_512mbt_16lpt_sharegpt_Qwen_Qwen2-7B-Instruct
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B-Instruct
    runs: 1
    result_folder: qwen2-7b-instruct
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 512
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 30000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 16
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline2:
    name: exp_100p_16r_0.0t_512mbt_256lpt_sharegpt_Qwen_Qwen2-7B-Instruct
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B-Instruct
    runs: 1
    result_folder: qwen2-7b-instruct
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 512
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 30000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 16
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline3:
    name: exp_100p_16r_0.0t_1024mbt_16lpt_sharegpt_Qwen_Qwen2-7B-Instruct
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B-Instruct
    runs: 1
    result_folder: qwen2-7b-instruct
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 30000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 16
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline4:
    name: exp_100p_16r_0.0t_1024mbt_256lpt_sharegpt_Qwen_Qwen2-7B-Instruct
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B-Instruct
    runs: 1
    result_folder: qwen2-7b-instruct
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 30000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 16
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
