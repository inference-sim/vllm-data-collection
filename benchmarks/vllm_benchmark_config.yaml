# vLLM Benchmark Configuration

# Model to benchmark


# Generated test configurations
test:
  baseline:
    name: exp_100p_4r_0.0t_256mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 256
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 4
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline2:
    name: exp_100p_4r_0.0t_256mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 256
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 4
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline3:
    name: exp_100p_4r_0.0t_1024mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 4
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline4:
    name: exp_100p_4r_0.0t_1024mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 4
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline5:
    name: exp_100p_4r_0.0t_1024mbt_1024lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 1024
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 4
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline6:
    name: exp_100p_4r_0.0t_8192mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 4
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline7:
    name: exp_100p_4r_0.0t_8192mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 4
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline8:
    name: exp_100p_4r_0.0t_8192mbt_1024lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 1024
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 4
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline9:
    name: exp_100p_8r_0.0t_256mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 256
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 8
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline10:
    name: exp_100p_8r_0.0t_256mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 256
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 8
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline11:
    name: exp_100p_8r_0.0t_1024mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 8
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline12:
    name: exp_100p_8r_0.0t_1024mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 8
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline13:
    name: exp_100p_8r_0.0t_1024mbt_1024lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 1024
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 8
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline14:
    name: exp_100p_8r_0.0t_8192mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 8
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline15:
    name: exp_100p_8r_0.0t_8192mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 8
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline16:
    name: exp_100p_8r_0.0t_8192mbt_1024lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 1024
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 8
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline17:
    name: exp_100p_16r_0.0t_256mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 256
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 16
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline18:
    name: exp_100p_16r_0.0t_256mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 256
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 16
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline19:
    name: exp_100p_16r_0.0t_1024mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 16
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline20:
    name: exp_100p_16r_0.0t_1024mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 16
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline21:
    name: exp_100p_16r_0.0t_1024mbt_1024lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 1024
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 16
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline22:
    name: exp_100p_16r_0.0t_8192mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 16
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline23:
    name: exp_100p_16r_0.0t_8192mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 16
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline24:
    name: exp_100p_16r_0.0t_8192mbt_1024lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 1024
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 16
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline25:
    name: exp_100p_32r_0.0t_256mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 256
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 32
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline26:
    name: exp_100p_32r_0.0t_256mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 256
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 32
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline27:
    name: exp_100p_32r_0.0t_1024mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 32
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline28:
    name: exp_100p_32r_0.0t_1024mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 32
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline29:
    name: exp_100p_32r_0.0t_1024mbt_1024lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 1024
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 32
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline30:
    name: exp_100p_32r_0.0t_8192mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 32
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline31:
    name: exp_100p_32r_0.0t_8192mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 32
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline32:
    name: exp_100p_32r_0.0t_8192mbt_1024lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 1024
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 32
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline33:
    name: exp_100p_64r_0.0t_256mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 256
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 64
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline34:
    name: exp_100p_64r_0.0t_256mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 256
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 64
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline35:
    name: exp_100p_64r_0.0t_1024mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 64
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline36:
    name: exp_100p_64r_0.0t_1024mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 64
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline37:
    name: exp_100p_64r_0.0t_1024mbt_1024lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 1024
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 64
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline38:
    name: exp_100p_64r_0.0t_8192mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 64
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline39:
    name: exp_100p_64r_0.0t_8192mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 64
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline40:
    name: exp_100p_64r_0.0t_8192mbt_1024lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 1024
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 100
      request_rate: 64
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline41:
    name: exp_400p_4r_0.0t_256mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 256
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 400
      request_rate: 4
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline42:
    name: exp_400p_4r_0.0t_256mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 256
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 400
      request_rate: 4
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline43:
    name: exp_400p_4r_0.0t_1024mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 400
      request_rate: 4
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline44:
    name: exp_400p_4r_0.0t_1024mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 400
      request_rate: 4
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline45:
    name: exp_400p_4r_0.0t_1024mbt_1024lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 1024
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 400
      request_rate: 4
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline46:
    name: exp_400p_4r_0.0t_8192mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 400
      request_rate: 4
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline47:
    name: exp_400p_4r_0.0t_8192mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 400
      request_rate: 4
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline48:
    name: exp_400p_4r_0.0t_8192mbt_1024lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 1024
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 400
      request_rate: 4
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline49:
    name: exp_400p_8r_0.0t_256mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 256
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 400
      request_rate: 8
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline50:
    name: exp_400p_8r_0.0t_256mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 256
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 400
      request_rate: 8
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline51:
    name: exp_400p_8r_0.0t_1024mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 400
      request_rate: 8
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline52:
    name: exp_400p_8r_0.0t_1024mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 400
      request_rate: 8
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline53:
    name: exp_400p_8r_0.0t_1024mbt_1024lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 1024
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 400
      request_rate: 8
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline54:
    name: exp_400p_8r_0.0t_8192mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 400
      request_rate: 8
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline55:
    name: exp_400p_8r_0.0t_8192mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 400
      request_rate: 8
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline56:
    name: exp_400p_8r_0.0t_8192mbt_1024lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 1024
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 400
      request_rate: 8
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline57:
    name: exp_400p_16r_0.0t_256mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 256
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 400
      request_rate: 16
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline58:
    name: exp_400p_16r_0.0t_256mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 256
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 400
      request_rate: 16
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline59:
    name: exp_400p_16r_0.0t_1024mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 400
      request_rate: 16
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline60:
    name: exp_400p_16r_0.0t_1024mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 400
      request_rate: 16
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline61:
    name: exp_400p_16r_0.0t_1024mbt_1024lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 1024
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 400
      request_rate: 16
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline62:
    name: exp_400p_16r_0.0t_8192mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 400
      request_rate: 16
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline63:
    name: exp_400p_16r_0.0t_8192mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 400
      request_rate: 16
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline64:
    name: exp_400p_16r_0.0t_8192mbt_1024lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 1024
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 400
      request_rate: 16
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline65:
    name: exp_400p_32r_0.0t_256mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 256
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 400
      request_rate: 32
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline66:
    name: exp_400p_32r_0.0t_256mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 256
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 400
      request_rate: 32
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline67:
    name: exp_400p_32r_0.0t_1024mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 400
      request_rate: 32
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline68:
    name: exp_400p_32r_0.0t_1024mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 400
      request_rate: 32
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline69:
    name: exp_400p_32r_0.0t_1024mbt_1024lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 1024
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 400
      request_rate: 32
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline70:
    name: exp_400p_32r_0.0t_8192mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 400
      request_rate: 32
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline71:
    name: exp_400p_32r_0.0t_8192mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 400
      request_rate: 32
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline72:
    name: exp_400p_32r_0.0t_8192mbt_1024lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 1024
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 400
      request_rate: 32
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline73:
    name: exp_400p_64r_0.0t_256mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 256
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 400
      request_rate: 64
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline74:
    name: exp_400p_64r_0.0t_256mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 256
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 400
      request_rate: 64
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline75:
    name: exp_400p_64r_0.0t_1024mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 400
      request_rate: 64
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline76:
    name: exp_400p_64r_0.0t_1024mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 400
      request_rate: 64
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline77:
    name: exp_400p_64r_0.0t_1024mbt_1024lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 1024
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 400
      request_rate: 64
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline78:
    name: exp_400p_64r_0.0t_8192mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 400
      request_rate: 64
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline79:
    name: exp_400p_64r_0.0t_8192mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 400
      request_rate: 64
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline80:
    name: exp_400p_64r_0.0t_8192mbt_1024lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 1024
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 400
      request_rate: 64
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline81:
    name: exp_800p_4r_0.0t_256mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 256
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 800
      request_rate: 4
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline82:
    name: exp_800p_4r_0.0t_256mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 256
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 800
      request_rate: 4
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline83:
    name: exp_800p_4r_0.0t_1024mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 800
      request_rate: 4
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline84:
    name: exp_800p_4r_0.0t_1024mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 800
      request_rate: 4
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline85:
    name: exp_800p_4r_0.0t_1024mbt_1024lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 1024
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 800
      request_rate: 4
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline86:
    name: exp_800p_4r_0.0t_8192mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 800
      request_rate: 4
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline87:
    name: exp_800p_4r_0.0t_8192mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 800
      request_rate: 4
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline88:
    name: exp_800p_4r_0.0t_8192mbt_1024lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 1024
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 800
      request_rate: 4
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline89:
    name: exp_800p_8r_0.0t_256mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 256
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 800
      request_rate: 8
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline90:
    name: exp_800p_8r_0.0t_256mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 256
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 800
      request_rate: 8
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline91:
    name: exp_800p_8r_0.0t_1024mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 800
      request_rate: 8
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline92:
    name: exp_800p_8r_0.0t_1024mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 800
      request_rate: 8
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline93:
    name: exp_800p_8r_0.0t_1024mbt_1024lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 1024
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 800
      request_rate: 8
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline94:
    name: exp_800p_8r_0.0t_8192mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 800
      request_rate: 8
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline95:
    name: exp_800p_8r_0.0t_8192mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 800
      request_rate: 8
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline96:
    name: exp_800p_8r_0.0t_8192mbt_1024lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 1024
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 800
      request_rate: 8
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline97:
    name: exp_800p_16r_0.0t_256mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 256
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 800
      request_rate: 16
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline98:
    name: exp_800p_16r_0.0t_256mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 256
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 800
      request_rate: 16
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline99:
    name: exp_800p_16r_0.0t_1024mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 800
      request_rate: 16
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline100:
    name: exp_800p_16r_0.0t_1024mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 800
      request_rate: 16
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline101:
    name: exp_800p_16r_0.0t_1024mbt_1024lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 1024
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 800
      request_rate: 16
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline102:
    name: exp_800p_16r_0.0t_8192mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 800
      request_rate: 16
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline103:
    name: exp_800p_16r_0.0t_8192mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 800
      request_rate: 16
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline104:
    name: exp_800p_16r_0.0t_8192mbt_1024lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 1024
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 800
      request_rate: 16
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline105:
    name: exp_800p_32r_0.0t_256mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 256
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 800
      request_rate: 32
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline106:
    name: exp_800p_32r_0.0t_256mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 256
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 800
      request_rate: 32
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline107:
    name: exp_800p_32r_0.0t_1024mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 800
      request_rate: 32
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline108:
    name: exp_800p_32r_0.0t_1024mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 800
      request_rate: 32
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline109:
    name: exp_800p_32r_0.0t_1024mbt_1024lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 1024
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 800
      request_rate: 32
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline110:
    name: exp_800p_32r_0.0t_8192mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 800
      request_rate: 32
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline111:
    name: exp_800p_32r_0.0t_8192mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 800
      request_rate: 32
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline112:
    name: exp_800p_32r_0.0t_8192mbt_1024lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 1024
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 800
      request_rate: 32
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline113:
    name: exp_800p_64r_0.0t_256mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 256
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 800
      request_rate: 64
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline114:
    name: exp_800p_64r_0.0t_256mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 256
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 800
      request_rate: 64
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline115:
    name: exp_800p_64r_0.0t_1024mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 800
      request_rate: 64
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline116:
    name: exp_800p_64r_0.0t_1024mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 800
      request_rate: 64
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline117:
    name: exp_800p_64r_0.0t_1024mbt_1024lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 1024
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 800
      request_rate: 64
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline118:
    name: exp_800p_64r_0.0t_8192mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 800
      request_rate: 64
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline119:
    name: exp_800p_64r_0.0t_8192mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 800
      request_rate: 64
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline120:
    name: exp_800p_64r_0.0t_8192mbt_1024lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 1024
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 800
      request_rate: 64
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline121:
    name: exp_1000p_4r_0.0t_256mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 256
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 1000
      request_rate: 4
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline122:
    name: exp_1000p_4r_0.0t_256mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 256
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 1000
      request_rate: 4
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline123:
    name: exp_1000p_4r_0.0t_1024mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 1000
      request_rate: 4
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline124:
    name: exp_1000p_4r_0.0t_1024mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 1000
      request_rate: 4
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline125:
    name: exp_1000p_4r_0.0t_1024mbt_1024lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 1024
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 1000
      request_rate: 4
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline126:
    name: exp_1000p_4r_0.0t_8192mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 1000
      request_rate: 4
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline127:
    name: exp_1000p_4r_0.0t_8192mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 1000
      request_rate: 4
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline128:
    name: exp_1000p_4r_0.0t_8192mbt_1024lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 1024
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 1000
      request_rate: 4
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline129:
    name: exp_1000p_8r_0.0t_256mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 256
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 1000
      request_rate: 8
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline130:
    name: exp_1000p_8r_0.0t_256mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 256
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 1000
      request_rate: 8
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline131:
    name: exp_1000p_8r_0.0t_1024mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 1000
      request_rate: 8
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline132:
    name: exp_1000p_8r_0.0t_1024mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 1000
      request_rate: 8
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline133:
    name: exp_1000p_8r_0.0t_1024mbt_1024lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 1024
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 1000
      request_rate: 8
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline134:
    name: exp_1000p_8r_0.0t_8192mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 1000
      request_rate: 8
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline135:
    name: exp_1000p_8r_0.0t_8192mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 1000
      request_rate: 8
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline136:
    name: exp_1000p_8r_0.0t_8192mbt_1024lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 1024
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 1000
      request_rate: 8
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline137:
    name: exp_1000p_16r_0.0t_256mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 256
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 1000
      request_rate: 16
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline138:
    name: exp_1000p_16r_0.0t_256mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 256
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 1000
      request_rate: 16
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline139:
    name: exp_1000p_16r_0.0t_1024mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 1000
      request_rate: 16
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline140:
    name: exp_1000p_16r_0.0t_1024mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 1000
      request_rate: 16
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline141:
    name: exp_1000p_16r_0.0t_1024mbt_1024lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 1024
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 1000
      request_rate: 16
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline142:
    name: exp_1000p_16r_0.0t_8192mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 1000
      request_rate: 16
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline143:
    name: exp_1000p_16r_0.0t_8192mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 1000
      request_rate: 16
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline144:
    name: exp_1000p_16r_0.0t_8192mbt_1024lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 1024
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 1000
      request_rate: 16
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline145:
    name: exp_1000p_32r_0.0t_256mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 256
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 1000
      request_rate: 32
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline146:
    name: exp_1000p_32r_0.0t_256mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 256
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 1000
      request_rate: 32
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline147:
    name: exp_1000p_32r_0.0t_1024mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 1000
      request_rate: 32
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline148:
    name: exp_1000p_32r_0.0t_1024mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 1000
      request_rate: 32
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline149:
    name: exp_1000p_32r_0.0t_1024mbt_1024lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 1024
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 1000
      request_rate: 32
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline150:
    name: exp_1000p_32r_0.0t_8192mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 1000
      request_rate: 32
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline151:
    name: exp_1000p_32r_0.0t_8192mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 1000
      request_rate: 32
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline152:
    name: exp_1000p_32r_0.0t_8192mbt_1024lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 1024
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 1000
      request_rate: 32
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline153:
    name: exp_1000p_64r_0.0t_256mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 256
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 1000
      request_rate: 64
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline154:
    name: exp_1000p_64r_0.0t_256mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 256
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 1000
      request_rate: 64
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline155:
    name: exp_1000p_64r_0.0t_1024mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 1000
      request_rate: 64
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline156:
    name: exp_1000p_64r_0.0t_1024mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 1000
      request_rate: 64
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline157:
    name: exp_1000p_64r_0.0t_1024mbt_1024lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 1024
      max_num_seqs: 256
      long_prefill_token_threshold: 1024
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 1000
      request_rate: 64
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline158:
    name: exp_1000p_64r_0.0t_8192mbt_16lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 16
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 1000
      request_rate: 64
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline159:
    name: exp_1000p_64r_0.0t_8192mbt_256lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 256
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 1000
      request_rate: 64
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42
  baseline160:
    name: exp_1000p_64r_0.0t_8192mbt_1024lpt_sharegpt_Qwen_Qwen2-7B
    description: Basic vLLM performance test
    model: Qwen/Qwen2-7B
    runs: 1
    result_folder: qwen2-7b
    vllm:
      gpu_memory_utilization: 0.9
      enable_prefix_caching: true
      disable_log_requests: false
      block_size: 16
      max_model_len: 2048
      max_num_batched_tokens: 8192
      max_num_seqs: 256
      long_prefill_token_threshold: 1024
      seed: 42
      gpu_type: NVIDIA-H100-80GB-HBM3
      gpu_memory_min: 50000
    benchmark:
      backend: vllm
      dataset_name: sharegpt
      dataset_path: ShareGPT_V3_unfiltered_cleaned_split.json
      num_prompts: 1000
      request_rate: 64
      sharegpt_output_len: 0
      temperature: 0.0
      seed: 42